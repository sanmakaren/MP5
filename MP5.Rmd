---
title: "SDS/CSC 293 Mini-Project 5: LASSO"
author: "Group 18: Karen Santamaria, Mariama Jaiteh"
date: "Thursday, May 2^nd^, 2019"
output:
  html_document:
    highlight: tango
    theme: cosmo
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: true
    df_print: kable
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Load all your packages here:
library(tidyverse)
library(glmnet)
library(modelr)
library(broom)
library(skimr)
library(Metrics)
library(randomForest)
library(Hmisc)
library(missForest)

# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE,
  fig.width = 16/2, fig.height = 9/2
)

# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```

You will be submiting an entry to Kaggle's [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/){target="_blank"} by fitting a fitted ~~spline~~, ~~multiple regression~~ LASSO regularized multiple regression model $\hat{f}(x)$.

However of the original 1460 rows of the `training` data, in the `data/` folder you are given a `train.csv` consisting of only 50 of the rows!



***



# Load data

Read in data provided by Kaggle for this competition. They are organized in the `data/` folder of this RStudio project:

```{r}
training <- read_csv("data/train.csv") %>% 
  rename(
    FirstFlrSF = `1stFlrSF`,
    SecondFlrSF = `2ndFlrSF`,
    ThirdSsnPorch = `3SsnPorch`
  ) %>% 
  # Fit your models to this outcome variable:
  mutate(log_SalePrice = log(SalePrice+1))

test <- read_csv("data/test.csv")%>% 
  rename(
    FirstFlrSF = `1stFlrSF`,
    SecondFlrSF = `2ndFlrSF`,
    ThirdSsnPorch = `3SsnPorch`
  )
sample_submission <- read_csv("data/sample_submission.csv")

# Function that takes in a LASSO fit object and returns a "tidy" data frame of
# the beta-hat coefficients for each lambda value used in LASSO fit. 
get_LASSO_coefficients <- function(LASSO_fit){
  beta_hats <- LASSO_fit %>%
    broom::tidy(return_zeros = TRUE) %>%
    select(term, estimate, lambda) %>%
    arrange(desc(lambda))
  return(beta_hats)
}



#data imputation 

training.mis <- prodNA(training, noNA = 0.1)

for (i in names(training)){
  if(is.numeric(training[[i]])){
    training.mis[[i]] <- with(training, impute(training[[i]], median))
  }
}

test.mis <- prodNA(test, noNA = 0.1)

for (i in names(test)){
  if(is.numeric(test[[i]])){
    test.mis[[i]] <- with(test, impute(test[[i]], median))
  }
}


```


## Look at your data!

Always, ALWAYS, **ALWAYS** start by looking at your raw data. This gives you visual sense of what information you have to help build your predictive models. To get a full description of each variable, read the data dictionary in the `data_description.txt` file in the `data/` folder.

Note that the following code chunk has `eval = FALSE` meaning "don't evaluate this chunk with knitting" because `.Rmd` files won't knit if they include a `View()`:

```{r, eval = FALSE}
View(training)
glimpse(training)

View(test)
glimpse(test)

# Pay close attention to the variables and variable types in sample_submission. 
# Your submission must match this exactly.
glimpse(sample_submission)

# Hint:
skim(training)
skim(test)
```



***



# Minimally viable product

Since we have already performed exploratory data analyses of this data in MP1 and MP2, let's jump straight into the modeling. For this phase:

* Train an unregularized standard multiple regression model $\widehat{f}_1$ using **all** 36 numerical variables as predictors.


```{r}
# Train your model here:

# Model formula
model_formula <- "log_SalePrice ~ MSSubClass + LotFrontage + LotArea + 
OverallQual + OverallCond + YearBuilt + YearRemodAdd + MasVnrArea + BsmtFinSF1 + 
BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + FirstFlrSF + SecondFlrSF + LowQualFinSF + 
GrLivArea + BsmtFullBath + BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + 
KitchenAbvGr + TotRmsAbvGrd + Fireplaces + GarageYrBlt + GarageCars + GarageArea + 
WoodDeckSF + OpenPorchSF + EnclosedPorch + ThirdSsnPorch + ScreenPorch + PoolArea + 
MiscVal + MoSold + YrSold" %>% 
  as.formula()

```

```{r}
# 1. Fit model to training data
model_1 <- lm(model_formula, data = training.mis)
```

```{r}
fitted_points_1 <- model_1 %>%
  broom::augment()
```

```{r}
predicted_points_1 <- model_1 %>%
  broom::augment(newdata = training.mis)
```


***



# Due diligence

* Compute two RMLSE's of the fitted model $\widehat{f}_1$
      a) on the `training` data. You may use a function from a package to achieve this.
      b) on the `test` data via a submission to Kaggle `data/submit_regression.csv`.
* Compare the two RMLSE's. If they are different, comment on why they might be different.

```{r}
# Compute both RMLSE's here:
rmsle_train = MLmetrics::RMSLE(y_pred = predicted_points_1$.fitted, y_true = predicted_points_1$log_SalePrice)
rmsle_train
```

```{r}
predict_test <- model_1 %>%
  broom::augment(newdata = test.mis)
  
```


```{r}
submission <- test %>%
  mutate(SalePrice = exp(predict_test$.fitted)) %>%
  select(Id, SalePrice)

write_csv(submission, path = "data/submission_due_diligence.csv")
```


RMLSE on training  | RMLSE on test (via Kaggle)
------------- | -------------
X             | Y




***



# Reaching for the stars

1. Find the $\lambda^*$ tuning parameter that yields the LASSO model with the
lowest estimated RMLSE as well as this lowest RMLSE as well. You may use functions included in a package for this.
1. Convince yourself with a visualization that the $\lambda^*$ you found is indeed the one that returns the lowest estimated RMLSE.
1. What is the model $\widehat{f}$_2 resulting from this $\lambda^*$? Output a data frame of the $\widehat{\beta}$.
1. Visualize the progression of $\widehat{\beta}$ for different $\lambda$ values and mark $\lambda^*$ with a vertical line:

```{r, results="hide"}
# Find lambda star:

x_matrix <- training.mis %>%
  modelr::model_matrix(model_formula, data = .) %>%
  select(-`(Intercept)`) %>%
as.matrix()


#lambda_inputs <- seq(from = 0, to = 1000)
lambda_inputs <- 10^seq(from = -5, to = 3, length = 100)

LASSO_fit <- glmnet(x = x_matrix, y = training$log_SalePrice, alpha = 1, lambda = lambda_inputs)

LASSO_CV <- cv.glmnet(
  x = x_matrix,
  y = training$log_SalePrice,
  alpha = 1,
  lambda = lambda_inputs,
  nfolds = 10,
  type.measure = "mse"
)

LASSO_CV %>%
  broom::tidy() %>%
  rename(mse = estimate) %>%
  arrange(mse)

# The lambda_star is in the top row. We can extract this lambda_star value from
# the LASSO_CV object:
lambda_star <- LASSO_CV$lambda.min
```


```{r}
lambda_star
```


Get visualization that confirms lambda_star is best lamba
```{r}
LASSO_CV_df <- LASSO_CV %>%
  broom::tidy() %>%
  rename(mse = estimate) %>%
  arrange(mse) 

LASSO_CV_df %>%
  ggplot(aes(x = lambda)) +
  geom_point(aes(y = mse)) +
  scale_x_log10() +
  labs(x = "lambda (log10-scale)", y = "Estimated RMSLE") +
  geom_vline(xintercept = lambda_star, linetype = "dashed", col = "blue")
```


Beta coeffiecents of best model
```{r}
get_LASSO_coefficients(LASSO_fit) %>%
  filter(lambda == lambda_star)
```



```{r}
# Visualize the progression of beta-hats for different lambda values and mark lambda_star with a vertical line:
LASSO_coefficients_plot <- get_LASSO_coefficients(LASSO_fit) %>%
  filter(term != "(Intercept)") %>%
  # Plot:
  ggplot(aes(x = lambda, y = estimate, col = term)) +
  geom_line() +
  labs(x = "lambda (log10-scale)", y = "beta-hat") +
  scale_x_log10() 

LASSO_coefficients_plot + 
  geom_vline(xintercept = lambda_star, color = "black", linetype = "dashed")
```



***



# Point of diminishing returns

1. In qualitative language, comment on the resulting amoung of shrinkage in the LASSO model?
1. Obtain the RMLSE of the fitted model
      a) on the `training` data
      b) on the `test` data via a submission to Kaggle `data/submit_LASSO.csv` that we will test.
1. Compare the two RMLSE's. If they are different, comment on why they might be different.

```{r}
# Compute both RMLSE's here:

#RMSLE for training
min(LASSO_CV_df$mse)

LASSO_fit_train <- glmnet(x = x_matrix, y = training$log_SalePrice, alpha = 1, lambda = lambda_star)

x_matrix_test <- test.mis %>%
  # Create temporary outcome variance just to get model matrix to work:
  mutate(log_SalePrice = 1) %>%
  modelr::model_matrix(model_formula, data = .) %>%
  select(-`(Intercept)`) %>%
as.matrix()

# Predict y_hat's for test data using model and same lambda = 10.
test.mis <- test.mis %>%
  mutate(y_hat_LASSO = predict(LASSO_fit_train, newx = x_matrix_test, s = 10)[,1])

submission2 <- test.mis %>%
  mutate(SalePrice = exp(test.mis$y_hat_LASSO)) %>%
  select(Id, SalePrice)

write_csv(submission, path = "data/submit_LASSO.csv")
```

Comparing both RMLSE's here:

Method           | RMLSE on training  | RMLSE on test (via Kaggle)
---------------- | ------------------ | -------------
Unregularized lm | X                  | Y
LASSO            | A                  | B



***


# Polishing the cannonball

1. Fit a LASSO model $\widehat{f}_3$ that uses categorical variables as well.
1. Output a `data/submit_LASSO_2.csv`
1. Submit to Kaggle and replace the screenshot below with an screenshot of your score.
1. Try to get the best Kaggle leaderboard score!



![](score_screenshot.png){ width=100% }





